# Tweet Thread: Training REINFORCE to Play Slither.io

ğŸ§µ 1/18
I trained a REINFORCE agent to play Slither.io! ğŸ

Instead of computer vision, I inject JavaScript to read the game's internal state and control the snake directly. The bot learns through trial and error using policy gradient RL.

Here's how it works ğŸ‘‡

---

ğŸ§µ 2/18
The project has two approaches:

1ï¸âƒ£ Rule-based: Hand-crafted logic (flee if enemy < 300 units, otherwise seek food)
2ï¸âƒ£ REINFORCE: Neural network that learns from experience

Both use Selenium to control the browser and JavaScript injection to read game state.

---

ğŸ§µ 3/18
Key insight: Slither.io stores everything in global JS variables!

-   `window.snake` = player's snake
-   `window.slithers` = all snakes (enemies)
-   `window.foods` = food pellets
-   `window.preys` = high-value food from dead snakes

We read these directly - no vision needed! ğŸ¯

---

ğŸ§µ 4/18
Critical improvement: Check the ENTIRE snake body for collisions, not just the head!

Collisions can happen with any body segment. We iterate through all `pts` (body points) to find the closest distance. Much safer than head-only detection.

---

ğŸ§µ 5/18
Actions are executed by manipulating game variables:

```javascript
window.snake.ang = angle_radians;
window.xm = offset_x;
window.ym = offset_y;
document.dispatchEvent(mousemove_event);
```

8 discrete directions: 0Â°, 45Â°, 90Â°, 135Â°, 180Â°, 225Â°, 270Â°, 315Â°

---

ğŸ§µ 6/18
State representation (11 dimensions):

-   Current angle & snake length
-   Nearest food distance & angle
-   Nearest prey distance & angle
-   Nearest enemy distance & angle
-   Counts of nearby foods/preys/enemies

All normalized to [-1, 1] for stable training.

---

ğŸ§µ 7/18
What is REINFORCE? ğŸ“

A policy gradient algorithm that:

1. Plays an episode with current policy
2. Collects rewards
3. Computes returns (discounted future rewards)
4. Updates policy to increase probability of high-return actions

It directly optimizes the policy - no value function needed!

---

ğŸ§µ 8/18
The policy network: Simple 3-layer MLP

Input: 11D state vector
Hidden: 128 â†’ 128 (ReLU)
Output: 8 action probabilities (softmax)

Takes state â†’ outputs probability distribution over actions â†’ sample action â†’ execute!

---

ğŸ§µ 9/18
Reward design is crucial! ğŸ

-   +10 per unit length increase (food collection)
-   -2.5 per step (survival penalty)
-   -50 for dying + 0.5 Ã— final length

Balances immediate rewards (food) with long-term goals (survival & growth).

---

ğŸ§µ 10/18
REINFORCE update step:

1. Compute discounted returns: R*t = r_t + Î³Â·r*{t+1} + Î³Â²Â·r\_{t+2} + ...
2. Normalize returns (subtract mean, divide by std) - reduces variance!
3. Policy loss: -log Ï€(a|s) Ã— R
4. Backprop & update

The math: âˆ‡J(Î¸) = E[âˆ‡log Ï€(a|s) Ã— R]

---

ğŸ§µ 11/18
Why normalization?

REINFORCE has HIGH variance. Baseline normalization (subtracting mean return) reduces variance without changing the expected gradient direction.

Makes training much more stable! ğŸ“ˆ

---

ğŸ§µ 12/18
Training process:

-   Online learning: Update after each episode
-   Each episode: Play until death or 1000 steps
-   Collect rewards & log-probabilities
-   Update policy using REINFORCE
-   Save best model when new max length achieved

---

ğŸ§µ 13/18
What the agent learns:

Early: Avoid enemies (steer away from nearby snakes)
Mid: Seek food (navigate toward pellets)
Later: Balance exploration/exploitation (chase prey vs avoid danger)

Learning curve is noisy (high variance) but improves over time!

---

ğŸ§µ 14/18
Comparison with rule-based policy:

Rule-based: Simple if-then logic (flee if enemy < 300 units)

RL agent potential:

-   Predict enemy movement patterns
-   Optimize paths to food while avoiding danger
-   Learn when to boost strategically

---

ğŸ§µ 15/18
Challenges:

âŒ High variance (single lucky/unlucky episode affects gradient)
âœ… Mitigation: Baseline normalization + discount factor

âŒ Sample inefficient (only updates after full episode)
ğŸ’¡ Future: PPO, Actor-Critic, experience replay

âŒ Real-world noise (network latency, unpredictable players)

---

ğŸ§µ 16/18
Key takeaways:

âœ… State extraction > computer vision (work with internal game state!)
âœ… Reward design shapes what agent learns
âœ… REINFORCE is simple but effective (great starting point)
âœ… Real-world RL is hard (variance, sample efficiency, noise)

---

ğŸ§µ 17/18
Tech stack:

-   PyTorch (neural networks)
-   Gymnasium (environment interface)
-   Selenium (browser automation)
-   3-layer MLP (128 hidden units)
-   Adam optimizer (lr=0.001)
-   Discount factor Î³=0.99

---

ğŸ§µ 18/18
Full blog post with code examples, detailed explanations, and results: [link]

Code on GitHub: [link]

Train your own agent: `python slither_rl.py`

#ReinforcementLearning #MachineLearning #Python #PyTorch #REINFORCE #SlitherIO #RL #AI

---

## Alternative Shorter Version (12 tweets)

ğŸ§µ 1/12
I trained REINFORCE to play Slither.io! ğŸ

Instead of vision, I inject JavaScript to read game state directly. The bot learns through trial and error using policy gradient RL.

Thread on how it works ğŸ‘‡

---

ğŸ§µ 2/12
Key insight: Slither.io stores state in global JS variables!

-   `window.snake` = player
-   `window.slithers` = enemies
-   `window.foods` = food pellets

We read these directly - no computer vision needed! ğŸ¯

---

ğŸ§µ 3/12
Critical: Check ENTIRE snake body for collisions, not just head!

We iterate through all body segments to find closest distance. Much safer collision avoidance.

---

ğŸ§µ 4/12
State: 11D vector (angle, length, nearest food/prey/enemy distances & angles, counts)

Actions: 8 discrete directions (0Â°, 45Â°, 90Â°, ...)

Policy: 3-layer MLP (128 hidden units) â†’ softmax over actions

---

ğŸ§µ 5/12
REINFORCE = Policy gradient algorithm:

1. Play episode with current policy
2. Collect rewards
3. Compute returns (discounted future rewards)
4. Update policy: increase prob of high-return actions

Directly optimizes policy - no value function!

---

ğŸ§µ 6/12
Reward design:

-   +10 per unit length increase
-   -2.5 per step
-   -50 for dying + 0.5 Ã— final length

Balances immediate rewards (food) with long-term goals (survival).

---

ğŸ§µ 7/12
REINFORCE update:

1. Compute discounted returns R*t = r_t + Î³Â·r*{t+1} + ...
2. Normalize (subtract mean, divide by std) - reduces variance!
3. Loss: -log Ï€(a|s) Ã— R
4. Backprop & update

Math: âˆ‡J(Î¸) = E[âˆ‡log Ï€(a|s) Ã— R]

---

ğŸ§µ 8/12
Why normalization?

REINFORCE has HIGH variance. Baseline normalization reduces variance without changing expected gradient direction.

Makes training stable! ğŸ“ˆ

---

ğŸ§µ 9/12
Training: Online learning

-   Update after each episode
-   Play until death or 1000 steps
-   Collect rewards & log-probs
-   Update policy
-   Save best model on new max length

---

ğŸ§µ 10/12
What agent learns:

Early: Avoid enemies
Mid: Seek food  
Later: Balance exploration/exploitation

Learning curve is noisy but improves over time!

---

ğŸ§µ 11/12
Challenges:

âŒ High variance â†’ Baseline normalization
âŒ Sample inefficient â†’ Future: PPO, Actor-Critic
âŒ Real-world noise â†’ Network latency, unpredictable players

---

ğŸ§µ 12/12
Takeaways:

âœ… State extraction > vision
âœ… Reward design matters
âœ… REINFORCE simple but effective
âœ… Real-world RL is hard

Full blog post: [link]
Code: [link]

#ReinforcementLearning #MachineLearning #Python #PyTorch #REINFORCE
